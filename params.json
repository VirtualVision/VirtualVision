{"name":"Virtual Vision","tagline":"UTA Senior Design Project","body":"##Project Objective\r\nThe goal of our project was to add an eye tracking module to the Oculus Rift. To meet this goal we needed to...\r\n*Attach and use a camera inside a VR headset.\r\n*Do image processing and apply eye tracking software to determine the gaze of the eye.\r\n*Create an example application of the data.\r\n*Establish communication between all components.\r\n\r\n\r\nWe mounted a camera inside the  Oculus Rift and used a Raspberry Pi 2 to track the eye location and communicate with a PC using an Ethernet connection. We created a game in Unity to simulate the calibration process and show how to use the eye tracking data within an application. \r\n\r\n##Hardware\r\nWe fabricated custom Oculus Rift lens risers with a slot to insert the camera between the Oculus lens and the screen. The camera used was a Raspberry Pi Spy Camera with an added IR filter to get a better picture of the eye. We also mounted IR LED's on the Oculus Rift to get proper lighting. Picture of setup.\r\n\r\n##Software\r\nFor pupil tracking we used the [Swirski robust pupil tracker.](https://www.cl.cam.ac.uk/research/rainbow/projects/pupiltracking/) This is run on the Raspberry Pi and the data is sent through a socket using [ZeroMQ](http://zeromq.org/). The PC application uses this data when running a calibration routine and for determining the users gaze. The Unity game has in game targets for the user to look at during the calibration routine, and creates a graphical representation where the user is looking. Example pictures.\r\n\r\n##Visualizing Users Gaze\r\nAfter calibration, there are two modes of representing gaze, point and sector tracking. Point tracking attempts to be more accurate but and less reliable and sector tracking attempted to be less accurate but more reliable.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}