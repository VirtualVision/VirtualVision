<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Virtual Vision : UTA Senior Design Project">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Virtual Vision</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/VirtualVision/VirtualVision">View on GitHub</a>

          <h1 id="project_title">Virtual Vision</h1>
          <h2 id="project_tagline">UTA Senior Design Project</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/VirtualVision/VirtualVision/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/VirtualVision/VirtualVision/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h2>
<a id="project-objective" class="anchor" href="#project-objective" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project Objective</h2>

<p>The goal of our project was to add an eye tracking module to the Oculus Rift. To meet this goal we needed to...
*Attach and use a camera inside a VR headset.
*Do image processing and apply eye tracking software to determine the gaze of the eye.
*Create an example application of the data.
*Establish communication between all components.</p>

<p>We mounted a camera inside the  Oculus Rift and used a Raspberry Pi 2 to track the eye location and communicate with a PC using an Ethernet connection. We created a game in Unity to simulate the calibration process and show how to use the eye tracking data within an application. </p>

<h2>
<a id="hardware" class="anchor" href="#hardware" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hardware</h2>

<p>We fabricated custom Oculus Rift lens risers with a slot to insert the camera between the Oculus lens and the screen. The camera used was a Raspberry Pi Spy Camera with an added IR filter to get a better picture of the eye. We also mounted IR LED's on the Oculus Rift to get proper lighting. Picture of setup.</p>

<h2>
<a id="software" class="anchor" href="#software" aria-hidden="true"><span class="octicon octicon-link"></span></a>Software</h2>

<p>For pupil tracking we used the <a href="https://www.cl.cam.ac.uk/research/rainbow/projects/pupiltracking/">Swirski robust pupil tracker.</a> This is run on the Raspberry Pi and the data is sent through a socket using <a href="http://zeromq.org/">ZeroMQ</a>. The PC application uses this data when running a calibration routine and for determining the users gaze. The Unity game has in game targets for the user to look at during the calibration routine, and creates a graphical representation where the user is looking. Example pictures.</p>

<h2>
<a id="visualizing-users-gaze" class="anchor" href="#visualizing-users-gaze" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualizing Users Gaze</h2>

<p>After calibration, there are two modes of representing gaze, point and sector tracking. Point tracking attempts to be more accurate but and less reliable and sector tracking attempted to be less accurate but more reliable.</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Virtual Vision maintained by <a href="https://github.com/VirtualVision">VirtualVision</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
